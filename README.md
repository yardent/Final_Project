# Deep Learning- Project
Language Model using a Recurrent Neural Network

הלמידה העמוקה (Deep Learning) היא תחום מחקר בעולם המחשבים שמטרתו לחקות באופן ממוחשב את פעולת המוח האנושי. התחום הוא מרכזי בעולם האינטליגנציה המלאכותית. המיוחד במערכות למידה עמוקה הוא היכולת שלהן ללמוד ולהשתפר כל הזמן. ממש כמו המוח האנושי, ככל שמערכת כזו פועלת וככל שמשתמשים בה - היא משתפרת ו"יודעת" יותר. 

כחלק מכין לביצוע הלמידה העמוקה, ביצענו ניתוח לנתונים, תהליך המורכב מבחינה, ניקוי, ביצוע טרנספורמציה ומידול של הנתונים. זאת במטרה של הדגשה והבלטה של המידע השימושי, ושיפור תהליך הלמידה והסקת המסקנות. 

בתהליך ה-Deep Learning אנו מבצעים ניתוח נתונים שמתמקדת ביצירת מודל וגילוי ידע למטרות של חיזוי.
בפרויקט שלנו התבקשנו, באמצעות למידה עמוקה (Deep Learning) וחקר נתונים (data science), לחזות ולייצר טקסט הדומה לטקסט שנכתב על ידי אדם. בעבודתנו חקרנו תסריט של הסרט
 The wolf of wall street"", שנכתב על ידי Terence Winter.
המטרה המרכזית שלנו בתהליך היא יצירת משפטים כמה שיותר אמתיים ואותנטיים. 
בפרויקט זה נציג איך על ידי שימוש בתסריט עבר, בנינו מודל שיכול ליצר משפטים חדשים לבד בסגנון המשפטים הכתובים בתסריט. כלומר, לבנות מודל אשר יכול לבנות משפט בעל n מילים אם נביא לו dataset מסויים של משפטים. 
הסיכוי למשפט הוא הסיכוי של כל מילה להופיע אחרי כל המילים שהופיעו לפניה.
מודל זה יכול לעזור במגוון רחב של תחומים אשר כוללים למידה של שפה, לדוגמא, בשורת חיפוש של גוגל, הצגת המשך משפט צפוי על פי הטקסט שהמשתמש הזין לשורת החיפוש עד כה. בנוסף, נוכל גם ליצור משפטים חדשים.
כדי לממש זאת, אנו צריכים לזכור עבור כל מילה את הסיכוי שלה להופיע אחרי כל המילים שקיבלנו בdataset . לרוב המודלים יהיה קשה לייצג כזו תלות ארוכה עקב אילוצי חישוב וזכרון, הם מוגבלים להסתכל על כמות מוגבלת של מילים אחורה. RNN, בתיאוריה, יכול להתמודד עם זה.
לרשת נוירונים, יש שכבות נסתרות. לרוב, מצב השכבות החבויות שלנו מתבססות רק על הקלט: input -> hidden -> output.
שימוש ביכולת הזיכרון משנה את זה בכך שכעת השכבה הנסתרת לא תלויה רק בקלט אלא גם בחותמת הזמן שיש לקלט ובשכבה החבויה של חותמת הזמן האחרונה: 
(input + prev_hidden) -> hidden -> output
השכבה הנסתרת החוזרת, לומדת מה צריך לזכור בכל פעם.

הפרויקט:
1.	Choosing training set- כאשר התחלנו את הפרויקט, חיפשנו טקסט אשר מצד אחד לא יהיה קצר מידי ומצד שני, שיכיל משפטים הגיוניים בשפה האנגלית על מנת שהמודל שלנו יצליח להוציא תוצאות טובות.
כמו כן, היה לנו חשוב לעבוד עם טקסט מעניין ורלוונטי.
לאחר כמה נסיונות, נשארנו עם התסריט לסרט – The wolf of wall street, שנכתב על ידי Terence Winter.
2.	Training data and preprocessing – לאחר בחירת הטקסט עליו נרצה לאמן את המודל שלנו, היה צורך לבצע עליו עיבוד מקדים.
א.	Tokenize text- ניקח את התסריט כraw text, אבל אנו רוצים לבצע חיזוי על בסיס מילים, כלומר לחלק את הטקסט שלנו למשפטים וכל משפט למילים. 
בקוד שלנו נעזרנו בNLTK's, שמפרק את הטקסט למשפטים, וכל משפט למילים. הפונקציות הם sent_tokenize ו- word_tokenize.
הקטנו את כל האותיות כך שהמילה Hello והמילה hello, יחשבו כאחת, והורדנו כל מיני סימני פיסוק על מנת שלא יפריעו לנו בניתוח. לדוגמא: נגמר! או נגמר. ייוצגו כשני מילים שונות.
ב.	Remove infrequent words- בטקסט רגיל כמו בכל טקסט או שפה, קיימות הרבה מילים אשר מופיעות לעיתים רחוקות בלבד, מילים אלו הם כמו רעש בשביל המודל שלנו ולכן אנו בוחרים להוציא אותם. מילון גדול יכול לגרום למודל שלנו לעבוד לאט יותר, ומכיוון שאין לנו הרבה דוגמאות מהמילים הללו, לא נצליח ללמוד איך משתמשים בהם כמו שצריך, ממש כמו כאשר בני אדם לומדים מילים, צריך לפגוש את המילה כמה פעמים בהקשרים שונים על מנת באמת להבין את המשמעות של המילה.
מתוך 3722 מילים שמרנו במילון את החשובות ביותר, ועכשיו הוא מכיל 2000 מילים.
כל מילה שהגדרנו אותה כמילה שמופיעה מעט מידי פעמים, נכנסה למילון בתוך משפט כ- UNKNOW_TOKEN. המילה UNKNOW_TOKEN הופכת להיות מילה במילון ונחזה אותה כמו כל מילה אחרת.
המילים השכיחות ביותר בטקסט הם:
•	You- מופיע 989 פעמים
•	The- מופיע 752 פעמים
•	A- מופיע 517 פעמים






